---
title: "Recitation 8"
author: "Di Zhang"
date: "April 2, 2020"
output: html_document
---

##Bagging and Random Forest##

```{r}
#install.packages("randomForest")
#install.packages("gbm")

library(randomForest)
library(gbm)
library(MASS)
library(caret)
```

```{r}
train=sample(1:nrow(Boston),nrow(Boston)/2) #construct training data
```


```{r}
set.seed(1)
bag.boston=randomForest(medv~.,data=Boston, subset=train,mtry=13,importance=TRUE) #apply bagging on decision tree
bag.boston
```

```{r}
#calculate test error

yhat.bag=predict(bag.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
mean((yhat.bag-boston.test)^2)
```
```{r}
bag.boston=randomForest(medv~.,data=Boston, subset=train,mtry=13,importance=TRUE,ntree=25) #change the number of trees in bagging
yhat.bag=predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
```

```{r}
rf.boston=randomForest(medv~.,data=Boston, subset=train,mtry=6,importance=TRUE,ntree=25) #apply random forest
yhat.rf=predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
```

```{r}
importance(rf.boston) #check variable selection based on random forest
```

```{r}
varImpPlot(rf.boston)
```

##Boosting##
```{r}
set.seed(1)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.tree=5000,interaction.depth = 4) #fit a boosted tree
summary(boost.boston)
```

```{r}
#check marginal effect of variables rm and lstat

par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
```

```{r}
#calculate test error

yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
```

```{r}
#fit a boosted tree with shrinkage parameter=0.05

boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.tree=5000,interaction.depth = 4, shrinkage=0.05)
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
```

```{r}
#hyper-parameter tuning of gradient boosting 

grid <- expand.grid(n.trees = c(1000,1500), interaction.depth=c(1:3), shrinkage=c(0.05,0.1),n.minobsinnode=c(10))
ctrl <- trainControl(method  = "cv",number  = 5)
unwantedoutput <- capture.output(GBMModel <- train(medv~.,data = Boston[train,],
                  method = "gbm", trControl = ctrl, tuneGrid = grid))


```

```{r}
print(GBMModel)
```

```{r}
yhat.boost=predict(GBMModel,newdata=Boston[-train,],n.trees=1000)
mean((yhat.boost-boston.test)^2)
```

